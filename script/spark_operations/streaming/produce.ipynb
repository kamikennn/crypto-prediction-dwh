{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288667d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "SPARK_VERSION=\"3.1.1\"\n",
    "SPARK_MASTER_HOST=\"spark-master\"\n",
    "SPARK_MASTER_PORT=\"7077\"\n",
    "HIVE_METASTORE_HOST=\"192.168.10.14\"\n",
    "HIVE_METASTORE_PORT=9083\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "spark.cores.max: Max number of cores for a spark session, not max cores allocating to each worker.\n",
    "spark.executor.cores: Number of cores allocating to each worker. The number can be less than number of cores each worker has.\n",
    "spark.executor.memory: Memory size for each worker.\n",
    "\n",
    "E.g., 2 cores * 5 workers\n",
    "\n",
    "    case-1:\n",
    "        .config(\"spark.cores.max\", 3)\n",
    "        .config(\"spark.executor.cores\", \"1\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "        >> 3 executors are created and each executor has 1 core and 1g memory.\n",
    "        \n",
    "    case-2:\n",
    "        .config(\"spark.cores.max\", 3)\n",
    "        .config(\"spark.executor.cores\", \"3\")\n",
    "        .config(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "        >> Failed. \n",
    "        >> Since each worker has 2 cores, we can only set even number and less 2.\n",
    "        >> A executor is created in one spark worker.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SparkCassandraApp\")\n",
    "    .config(\n",
    "        \"spark.master\",\n",
    "        \"spark://{}:{}\".format(SPARK_MASTER_HOST, SPARK_MASTER_PORT),\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.hadoop.hive.metastore.uris\",\n",
    "        \"thrift://{}:{}\".format(HIVE_METASTORE_HOST, HIVE_METASTORE_PORT),\n",
    "    )\n",
    "    .config('spark.jars.packages', f'org.apache.spark:spark-sql-kafka-0-10_2.12:{SPARK_VERSION}')\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "    .config(\"spark.cores.max\", 1)\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.debug.maxToStringFields\", \"100\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b01936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"192.168.10.4:9081,192.168.10.4:9082,192.168.10.4:9083\"\n",
    "kafka_topic = \"crypto.candles_minute\"\n",
    "output_kafka_topic = \"spark.test_1\"\n",
    "\n",
    "# Define options for reading data from Kafka\n",
    "kafka_source_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"startingOffsets\": \"latest\"  # You can specify where to start reading data from\n",
    "}\n",
    "\n",
    "# Read messages from Kafka a topic\n",
    "kafka_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_source_params) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process messages\n",
    "processed_stream_df = kafka_stream_df \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.functions import rand, round\n",
    "\n",
    "# processed_stream_df=processed_stream_df.withColumn(\"partition\",lit(5))\n",
    "# # processed_stream_df=processed_stream_df.withColumn(\"partition\",(round(rand(seed=23)*10, 0)).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define options for writing data back to Kafka\n",
    "kafka_sink_params = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"topic\": output_kafka_topic\n",
    "}\n",
    "\n",
    "\n",
    "# Write the processed data back to Kafka\n",
    "write_stream = processed_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_sink_params) \\\n",
    "    .option(\"checkpointLocation\", \"/home/tmp/checkpoints\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f33ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b6d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33875017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fd4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b394cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbfdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea73d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78659a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee746e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c20aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
